
# Stack AI - Vector DB

## Objective

The goal of this project is to develop a REST API that allows users to index and query their documents within a Vector Database. A Vector Database specializes in storing and indexing vector embeddings, enabling fast retrieval and similarity searches. This capability is crucial for applications involving natural language processing, recommendation systems, and more.

The REST API should be containerized in a Docker container and deployed in a standalone Kubernetes cluster (no need to have more than one db node).

## Tasks

 - Allow the users to create, read, update, and delete libraries.
 - Allow the users to create, read, update and delete chunks within a library.
 - Index the contents of a library.
 - Do k-Nearest Neighbor vector search over the selected library with a given embedding query.

## Technologies Used
Python, Pydantic, FastAPI, SQLAlchemy, PostgreSQL, Docker, Minikube




## Steps
### Installation
- Git clone the repository to clone the project on your system.
```bash
  git clone https://github.com/Jay-Naruto/take-home-stack-ai.git .
```
    
- Open the directory in VS Code.
- Create a virtual environment and activate it.
```bash
  python3 -m venv venv
  source venv/bin/activate
```
- Install requirements.
```bash
  pip install -r requirements.txt
```
- Set up an .env file
TESTING=True is used for testing. DATABASE_URL doesn't have any password and the link can be used run the db locally on all the devices.
```bash
  TESTING=
  COHERE_API_KEY=
  DATABASE_URL=postgresql://postgres@postgres:5432/vectordb
```

- Make sure docker desktop is available (or you can download from this [link](https://www.docker.com/products/docker-desktop/)).

- Keep the docker desktop running in background.

- Run this command to build the application.
```bash
  docker-compose up --build
```
- You should see a link to `http://0.0.0.0:8000` in the terminal.

### Minikube deployment
- Install Minikube
```bash
  brew install minikube
```
- Install Helm
```bash
  brew install helm
```
- Start minikube
```bash
  minikube start
```
- Create helm chart using
```bash
  helm create stackai-chart
```
- Add these changes in stackai-chart/values.yaml
```bash
  image:
    repository: <docker-username>/<docker-image-name>
    ..
    tag: "latest"

    // rest of the code

    env:
    - name: DATABASE_URL
        value: ""
    - name: COHERE_API_KEY
        value: ""
    - name: TESTING
        value: ""
    
    service:
        type: ClusterIP
        port: 5432 
        name: postgres

    livenessProbe:
        httpGet:
            path: /
            port: 8000
    readinessProbe:
        httpGet:
            path: /
            port: 8000
```
- Add these changes in stackai-chart/templates/deployment.yaml
```bash
    ports:
        - name: http
          containerPort: {{ .Values.service.port }}
          protocol: TCP
    {{- if .Values.env }}
    env:
        {{- range .Values.env }}
        - name: {{ .name }}
          value: "{{ .value }}"
        {{- end }}
    {{- end }}
```
- Make this change in stackai-chart/templates/service.yaml
```bash
  targetPort: 8000
```

- Install the helm chart
```bash
  helm install stackai-chart ./stackai-chart
```
or upgrade
```bash
  helm upgrade --install stackai-chart ./stackai-chart
```

- Check if pods have been created
```bash
  kubectl get pods --namespace default
```
- Execute the commands given in terminal after helm install (upgrade)


## Database Structure

This project uses SQLAlchemy and PostgreSQL to manage documents, chunks of data, and associated metadata.

### Tables:
- **Libraries** (`LibraryDB`):
   - `id` (UUID): Unique identifier.
   - `name` (String): Library name.
   - `documents` (Relationship): One-to-many with `DocumentDB`.
   - `metadata_config` (JSON): Additional metadata.

- **Documents** (`DocumentDB`):
   - `id` (UUID): Unique identifier.
   - `chunks` (Relationship): One-to-many with `ChunkDB`.
   - `metadata_config` (JSON): Document metadata.
   - `library_id` (UUID): Foreign key to `LibraryDB`.

- **Chunks** (`ChunkDB`):
   - `id` (UUID): Unique identifier.
   - `text` (String): Chunk content.
   - `embedding` (Vector): Chunk vector for similarity search.
   - `metadata_config` (JSON): Chunk metadata.
   - `document_id` (UUID): Foreign key to `DocumentDB`.


### Pydantic Models:
- **Chunk**:
   - `id` (UUID) – **Optional** (default: generated by `uuid4()`)
   - `text` (String) – **Required**
   - `embedding` (List[float]) – **Optional** (generated by Cohere API)
   - `metadata_config` (Dict) – **Required**
   - `document_id` (UUID) – **Required**

- **Document**:
   - `id` (UUID) – **Optional** (default: generated by `uuid4()`)
   - `chunks` (List[Chunk]) – **Required**
   - `metadata_config` (Dict) – **Required**
   - `library_id` (UUID) – **Required**

- **Library**:
   - `id` (UUID) – **Optional** (default: generated by `uuid4()`)
   - `name` (String) – **Required**
   - `documents` (List[Document]) – **Required**
   - `metadata_config` (Dict) – **Optional** (default: empty dictionary)

- **Query**:
   - `text` (String) – **Required**

## Endpoints

### Library Endpoints:
- **POST /libraries/**: Create a new library.
- **GET /libraries/**: Get a list of all libraries.
- **GET /libraries/{library_id}**: Get a specific library by ID.
- **PUT /libraries/{library_id}**: Update a library by ID.
- **DELETE /libraries/{library_id}**: Delete a library by ID.

### Chunk Endpoints:
- **POST /libraries/{library_id}/documents/{document_id}/chunk**: Create a new chunk for a document.
- **GET /chunks**: Get a list of all chunks.
- **GET /chunk/{chunk_id}**: Get a specific chunk by ID.
- **PUT /libraries/{library_id}/documents/{document_id}/chunks/{chunk_id}**: Update a chunk by ID.
- **DELETE /libraries/{library_id}/documents/{document_id}/chunks/{chunk_id}**: Delete a chunk by ID.

### Search Endpoints:
- **POST /search/ball-tree/query**: Perform a search using a ball-tree algorithm.
- **POST /search/kd-tree/query**: Perform a search using a kd-tree algorithm.

## Indexing Algorithms

### KD-Tree
- **Definition**: A KD-Tree (k-dimensional tree) is a binary tree used to organize points in a k-dimensional space for efficient nearest neighbor search.
- **Why Used**: It partitions space into regions, reducing the search space for nearest neighbors in high-dimensional data.
- **Time Complexity**: 
  - **Build**: O(n log n)
  - **Query**: O(log n) on average, but can degrade to O(n) in the worst case.
- **Space Complexity**: O(n), where n is the number of data points.

### Ball Tree
- **Definition**: A Ball Tree is a binary tree where each node represents a cluster of points within a ball, defined by a center and radius, designed for efficient nearest neighbor search.
- **Why Used**: Effective for high-dimensional datasets, enabling fast pruning during nearest neighbor queries.
- **Time Complexity**: 
  - **Build**: O(n log n) on average.
  - **Query**: O(log n) on average, but can degrade in worst-case scenarios.
- **Space Complexity**: O(n), where n is the number of data points.


## Code Quality

This project meets all the outlined evaluation criteria:

- **SOLID Design Principles**: Follows SOLID principles for maintainable and scalable architecture.
- **Use of Static Typing**: Static typing through Python's `typing` module for better readability and error prevention.
- **FastAPI Good Practices**: Adheres to FastAPI best practices for efficient API development.
- **Pydantic Schema Validation**: Uses Pydantic for request/response validation ensuring correct data handling.
- **Code Modularity and Reusability**: Modular code structure for maintainability and reusability.
- **Use of RESTful API Endpoints**: Implements standard RESTful endpoints (GET, POST, PUT, DELETE).
- **Project Containerization with Docker**: Containerized with Docker for consistent deployment across environments.
- **Testing**: Includes unit and integration tests for stability and correctness.
- **Error Handling**: Comprehensive error handling with custom exceptions and appropriate status codes.
- **CRUD Operations**: Ensures atomicity, consistency, and integrity during all CRUD operations through the use of transactions.
- **Kubernetes Deployment**: Includes a Helm chart to install the project in a Kubernetes cluster, such as Minikube, for easy and efficient deployment.

## Demo
- [Installation and Demo](https://drive.google.com/file/d/16ZYGtn8n_SvPIJ3wwzQuvtBiXcLCNQzq/view?usp=sharing)

- [Explanation of technical choices](https://drive.google.com/file/d/1pyFhKPNkccHjPCIegFIfLCQ8R2_HmkfI/view?usp=sharing)

## Environment Variables

To run this project, you will need to add the following environment variables to your .env file

`TESTING`

`COHERE_API_KEY`

`DATABASE_URL`


